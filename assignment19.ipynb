{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Can we use Bagging for regression problems.\n",
        "\n",
        "\n",
        "Yes, **Bagging (Bootstrap Aggregating)** can be used for regression problems.\n",
        "\n",
        "## What is Bagging?\n",
        "\n",
        "Bagging is an ensemble learning technique where:\n",
        "\n",
        "* Multiple models are trained on different random samples (with replacement) of the dataset.\n",
        "* The final prediction is obtained by combining all model predictions.\n",
        "\n",
        "## How Bagging Works in Regression\n",
        "\n",
        "In regression problems:\n",
        "\n",
        "* Each model predicts a continuous value.\n",
        "* The final output is the **average** of all predictions.\n",
        "\n",
        "[\n",
        "Final\\ Prediction = \\frac{1}{N} \\sum_{i=1}^{N} Prediction_i\n",
        "]\n",
        "\n",
        "This averaging reduces variance and improves model stability.\n",
        "\n",
        "\n",
        "## Advantages of Bagging in Regression\n",
        "\n",
        "1. Reduces overfitting\n",
        "2. Decreases variance\n",
        "3. Improves prediction accuracy\n",
        "4. Works well with high-variance models like Decision Trees\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bk2IEBPbRXsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between multiple model training and single model training?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##  1. Single Model Training\n",
        "\n",
        "Single model training means training **only one machine learning model** on the dataset.\n",
        "\n",
        "###  Characteristics:\n",
        "\n",
        "* One algorithm is used.\n",
        "* One model is fitted to the data.\n",
        "* Final prediction comes from that single model.\n",
        "* Simple and faster to train.\n",
        "\n",
        "###  Example:\n",
        "\n",
        "Training one Decision Tree using **scikit-learn**.\n",
        "\n",
        "###  Advantages:\n",
        "\n",
        "* Easy to implement\n",
        "* Less computational cost\n",
        "* Faster training\n",
        "\n",
        "###  Disadvantages:\n",
        "\n",
        "* Can overfit easily\n",
        "* May have high bias or high variance\n",
        "* Less robust\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Multiple Model Training\n",
        "\n",
        "Multiple model training (Ensemble Learning) means training **more than one model** and combining their predictions.\n",
        "\n",
        "### Characteristics:\n",
        "\n",
        "* Multiple models are trained.\n",
        "* Each model learns from data (sometimes different subsets).\n",
        "* Final prediction is combined (average or voting).\n",
        "\n",
        "### Example:\n",
        "\n",
        "**Random Forest** trains many decision trees and averages their predictions.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "* Better accuracy\n",
        "* Reduces overfitting\n",
        "* More stable predictions\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "* More computational cost\n",
        "* Slower training\n",
        "* More complex\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Key Differences\n",
        "\n",
        "| Feature          | Single Model Training | Multiple Model Training |\n",
        "| ---------------- | --------------------- | ----------------------- |\n",
        "| Number of models | One                   | Many                    |\n",
        "| Accuracy         | Moderate              | Usually Higher          |\n",
        "| Overfitting risk | Higher                | Lower                   |\n",
        "| Complexity       | Simple                | Complex                 |\n",
        "| Example          | One Decision Tree     | Random Forest           |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m0CTNf_WSRIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Explain the concept of feature randomness in Random Forest?\n",
        "\n",
        "### Feature Randomness in Random Forest\n",
        "\n",
        "Feature randomness is one of the **key ideas** behind **Random Forest** that makes it powerful and reduces overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Feature Randomness?\n",
        "\n",
        "In Random Forest, **not all features are considered at every split** of a decision tree.\n",
        "\n",
        "Instead:\n",
        "\n",
        "* At each split,\n",
        "* A **random subset of features** is selected,\n",
        "* And the best split is chosen **only from that subset**, not from all features.\n",
        "\n",
        "This is called **feature randomness** (or feature bagging).\n",
        "\n",
        "---\n",
        "\n",
        "## Why is Feature Randomness Important?\n",
        "\n",
        "If we use all features for every split:\n",
        "\n",
        "* Many trees would look very similar.\n",
        "* Strong features would dominate every tree.\n",
        "* Trees become highly correlated.\n",
        "* The model may overfit.\n",
        "\n",
        "Feature randomness helps by:\n",
        "\n",
        " Making trees different from each other\n",
        " Reducing correlation between trees\n",
        " Improving generalization\n",
        " Reducing overfitting\n",
        "\n",
        "\n",
        "\n",
        "## How It Works (Example)\n",
        "\n",
        "Suppose your dataset has **10 features**.\n",
        "\n",
        "Instead of checking all 10 features at each split:\n",
        "\n",
        "* Random Forest might randomly select only **3 features**\n",
        "* Then choose the best split among those 3\n",
        "\n",
        "At the next split, it may select a **different random 3 features**.\n",
        "\n",
        "This randomness ensures each tree learns different patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "VOkfyCD_S33s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "Out-of-Bag (OOB) Score is a validation technique used in Random Forest to evaluate model performance without using a separate validation dataset.\n",
        "\n",
        "In Random Forest, each tree is trained on a bootstrap sample (random sampling with replacement). Some data points are not selected in that sample ‚Äî these are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "The model uses these OOB samples to test the corresponding tree, and the average performance across all trees is called the OOB score."
      ],
      "metadata": {
        "id": "JN_N7VxRFZbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "In **Random Forest**, feature importance measures how much each feature contributes to making accurate predictions. It can be measured in the following ways:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Mean Decrease in Impurity (MDI)\n",
        "\n",
        "Also called **Gini Importance** (for classification).\n",
        "\n",
        "* Each time a feature is used to split a node, it reduces impurity (Gini index or variance).\n",
        "* The total reduction in impurity caused by that feature across all trees is calculated.\n",
        "* Features with higher total reduction are considered more important.\n",
        "\n",
        "‚úî Built-in method in Random Forest\n",
        "‚úî Fast to compute\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Permutation Importance (Mean Decrease in Accuracy)\n",
        "\n",
        "* Randomly shuffle the values of one feature.\n",
        "* Measure how much the model accuracy decreases.\n",
        "* If accuracy drops significantly, the feature is important.\n",
        "\n",
        "‚úî More reliable\n",
        "‚úî Works for both classification and regression\n",
        "\n"
      ],
      "metadata": {
        "id": "0KsHTCiMFkuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "\n",
        "\n",
        "A **Bagging Classifier** (Bootstrap Aggregating) is an ensemble learning method that improves model performance by combining multiple base classifiers.\n",
        "\n",
        "The working principle is as follows:\n",
        "\n",
        "1Ô∏è‚É£ **Bootstrap Sampling**\n",
        "From the original dataset, multiple new training datasets are created by random sampling **with replacement**. Each of these datasets is called a bootstrap sample.\n",
        "\n",
        "2Ô∏è‚É£ **Training Multiple Models**\n",
        "A separate base classifier (usually a Decision Tree) is trained on each bootstrap sample independently.\n",
        "\n",
        "3Ô∏è‚É£ **Aggregation (Voting)**\n",
        "For classification problems, each model makes a prediction.\n",
        "The final output is decided by **majority voting** ‚Äî the class that receives the highest number of votes is selected as the final prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "BWQHC_foIADF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How do you evaluate a Bagging Classifier‚Äôs performance?\n",
        "\n",
        "\n",
        "\n",
        "The performance of a **Bagging Classifier** is evaluated using standard classification evaluation metrics. The common methods are:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Accuracy\n",
        "\n",
        "Measures the proportion of correctly predicted instances out of total instances.\n",
        "\n",
        "[\n",
        "Accuracy = \\frac{Correct\\ Predictions}{Total\\ Predictions}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Confusion Matrix\n",
        "\n",
        "Shows the number of:\n",
        "\n",
        "* True Positives (TP)\n",
        "* True Negatives (TN)\n",
        "* False Positives (FP)\n",
        "* False Negatives (FN)\n",
        "\n",
        "It helps understand model errors clearly.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Precision, Recall, and F1-Score\n",
        "\n",
        "* **Precision** ‚Üí Correct positive predictions out of total predicted positives\n",
        "* **Recall** ‚Üí Correct positive predictions out of actual positives\n",
        "* **F1-Score** ‚Üí Harmonic mean of Precision and Recall\n",
        "\n",
        "Useful when dealing with imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ ROC-AUC Score\n",
        "\n",
        "Measures the model‚Äôs ability to distinguish between classes.\n",
        "Higher AUC indicates better classification performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 5Ô∏è‚É£ Out-of-Bag (OOB) Score\n",
        "\n",
        "If bootstrap sampling is used (like in bagging), the OOB score can be used as an internal validation method without a separate test set.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vrx90gShIaka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.How does a Bagging Regressor work?\n",
        "\n",
        "A **Bagging Regressor** (Bootstrap Aggregating for regression) is an ensemble learning technique used to improve the accuracy and stability of regression models.\n",
        "\n",
        "Its working principle is as follows:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Bootstrap Sampling\n",
        "\n",
        "From the original dataset, multiple new training datasets are created using **random sampling with replacement**. Each dataset is called a bootstrap sample.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Training Multiple Base Regressors\n",
        "\n",
        "A separate regression model (commonly a Decision Tree Regressor) is trained independently on each bootstrap sample.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Aggregation (Averaging)\n",
        "\n",
        "For a new input, each trained regressor predicts an output value.\n",
        "The final prediction is calculated as the **average of all individual predictions**.\n",
        "\n",
        "[\n",
        "Final\\ Prediction = \\frac{Sum\\ of\\ all\\ model\\ predictions}{Number\\ of\\ models}\n",
        "]\n",
        "\n",
        "---\n",
        "\n",
        "###  Key Advantages:\n",
        "\n",
        "* Reduces variance\n",
        "* Decreases overfitting\n",
        "* Improves prediction stability\n",
        "* Works well with high-variance models\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JxHASTqLJFwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is the main advantage of ensemble techniques?\n",
        "\n",
        "\n",
        "The main advantage of **ensemble techniques** is that they **improve prediction accuracy and model reliability** by combining the outputs of multiple models instead of relying on a single model.\n",
        "\n",
        "By aggregating predictions from different learners, ensemble methods reduce errors caused by **bias, variance, or noise**, and help prevent **overfitting**. As a result, ensemble models usually perform better and generalize more effectively than individual models.\n"
      ],
      "metadata": {
        "id": "ykCRScMjJcyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "\n",
        "The main challenge of **ensemble methods** is their **increased computational complexity and reduced interpretability** compared to single models.\n",
        "\n",
        "Since ensemble techniques combine multiple models:\n",
        "\n",
        "* They require **more training time and memory**.\n",
        "* They are **computationally expensive**.\n",
        "* The final model becomes harder to interpret and explain.\n",
        "\n",
        "Because many models are combined, it is difficult to understand how individual features influence the final prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "IMsa-KIsJq8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "\n",
        "The key idea behind **ensemble techniques** is to **combine multiple models to produce a better and more accurate prediction than a single model**.\n",
        "\n",
        "Instead of relying on one model, ensemble methods train several base learners and combine their outputs using techniques such as **voting** (for classification) or **averaging** (for regression).\n",
        "\n",
        "This approach works on the principle that a group of weak or moderate learners, when combined properly, can form a strong learner. It helps in reducing bias, variance, and overfitting, leading to improved generalization performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "G2f36k51J2Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "\n",
        "A **Random Forest Classifier** is a supervised machine learning algorithm used for classification tasks. It is an ensemble method that builds multiple decision trees and combines their predictions to produce a final output.\n",
        "\n",
        "In **Random Forest**, each tree is trained on a random subset of the training data (bootstrap sampling), and at each split, a random subset of features is considered. This randomness helps create diverse trees.\n",
        "\n",
        "For classification problems, each tree gives a class prediction, and the final output is determined by **majority voting**.\n",
        "\n",
        "###  Key Advantages:\n",
        "\n",
        "* Reduces overfitting compared to a single decision tree\n",
        "* Handles large datasets and high-dimensional data\n",
        "* Provides feature importance\n",
        "* High accuracy and robustness\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RVGWBq85KPBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What are the main types of ensemble techniques?\n",
        "\n",
        "\n",
        "The main types of ensemble techniques are:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Bagging (Bootstrap Aggregating)\n",
        "\n",
        "In **Bagging**, multiple models are trained independently on different bootstrap samples of the dataset.\n",
        "The final prediction is made by:\n",
        "\n",
        "* **Majority voting** (classification)\n",
        "* **Averaging** (regression)\n",
        "\n",
        "Example: **Random Forest**\n",
        "\n",
        "‚úî Reduces variance\n",
        "‚úî Prevents overfitting\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Boosting\n",
        "\n",
        "In **Boosting**, models are trained sequentially, where each new model focuses on correcting the errors of the previous one.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* **AdaBoost**\n",
        "* **Gradient Boosting**\n",
        "* **XGBoost**\n",
        "\n",
        "‚úî Reduces bias\n",
        "‚úî Improves accuracy\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Stacking (Stacked Generalization)\n",
        "\n",
        "In **Stacking**, multiple different models are trained, and their predictions are combined using another model called a **meta-learner**.\n",
        "\n",
        "‚úî Combines strengths of different algorithms\n",
        "‚úî Often gives very high performance\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ek3HgK0CKdMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What is ensemble learning in machine learning?\n",
        "\n",
        "\n",
        "**Ensemble learning** is a machine learning technique in which multiple models (called base learners) are combined to improve overall prediction performance.\n",
        "\n",
        "Instead of relying on a single model, ensemble learning aggregates the predictions of several models using methods like:\n",
        "\n",
        "* **Voting** (for classification)\n",
        "* **Averaging** (for regression)\n",
        "\n",
        "The main idea is that a group of models working together can produce more accurate and stable results than an individual model.\n",
        "\n",
        "A popular example of ensemble learning is **Random Forest**, which combines multiple decision trees to make final predictions.\n",
        "\n",
        "###  Advantages:\n",
        "\n",
        "* Improves accuracy\n",
        "* Reduces overfitting\n",
        "* Enhances model stability\n",
        "* Better generalization\n",
        "\n"
      ],
      "metadata": {
        "id": "WOq_imnSLDRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.When should we avoid using ensemble methods?\n",
        "\n",
        "\n",
        "Although ensemble methods improve accuracy, they are not always the best choice. We should avoid using ensemble methods in the following situations:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ When Model Interpretability is Important\n",
        "\n",
        "Ensemble models combine multiple learners, making them complex and difficult to interpret.\n",
        "If explainability is required (e.g., in healthcare or finance), simpler models may be preferred.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Limited Computational Resources\n",
        "\n",
        "Ensemble methods require more memory, processing power, and training time compared to single models.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Small Datasets\n",
        "\n",
        "With very small datasets, ensemble methods may not provide significant improvement and can sometimes lead to overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ Real-Time or Low-Latency Systems\n",
        "\n",
        "If predictions must be made very quickly, complex ensembles may slow down performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 5Ô∏è‚É£ When a Simple Model Performs Well\n",
        "\n",
        "If a single model already provides high accuracy, using an ensemble may add unnecessary complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "2KA3aHCVLVNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.How does Bagging help in reducing overfitting?\n",
        "\n",
        "\n",
        "Bagging (Bootstrap Aggregating) reduces overfitting by lowering the **variance** of a machine learning model. It works by generating multiple bootstrap samples (random samples with replacement) from the original dataset and training separate models on each sample.\n",
        "\n",
        "Since each model is trained on slightly different data, they learn different patterns and make different errors. When their predictions are combined‚Äîusing averaging for regression or majority voting for classification‚Äîthe individual errors tend to cancel out.\n",
        "\n",
        "This aggregation makes the final model more stable and less sensitive to noise in the training data, thereby reducing overfitting and improving generalization performance on unseen data.\n"
      ],
      "metadata": {
        "id": "LfB1hcdOLzqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "\n",
        "**Random Forest** is better than a single Decision Tree because it reduces overfitting and improves prediction accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Reduces Overfitting\n",
        "\n",
        "A single Decision Tree can easily overfit the training data, especially if it is deep.\n",
        "Random Forest builds **multiple decision trees** and combines their predictions, which reduces variance and makes the model more stable.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Better Generalization\n",
        "\n",
        "Since Random Forest averages the results of many trees, it performs better on unseen (test) data compared to a single tree.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Feature Randomness\n",
        "\n",
        "Random Forest selects a **random subset of features** at each split.\n",
        "This makes trees less correlated with each other and improves overall performance.\n",
        "\n",
        "---\n",
        "\n",
        "###  4Ô∏è‚É£ Higher Accuracy\n",
        "\n",
        "By combining predictions from multiple trees (majority voting for classification, averaging for regression), Random Forest usually achieves higher accuracy than a single Decision Tree.\n",
        "\n",
        "---\n",
        "\n",
        "### 5Ô∏è‚É£ Robust to Noise\n",
        "\n",
        "If one tree makes an incorrect prediction due to noise, other trees can correct it during aggregation.\n",
        "\n"
      ],
      "metadata": {
        "id": "x952ActKMMGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the role of bootstrap sampling in Bagging?\n",
        "### Role of Bootstrap Sampling in Bagging\n",
        "\n",
        "Bootstrap sampling is a key component of **Bagging (Bootstrap Aggregating)**. It involves creating multiple training datasets by randomly sampling the original dataset **with replacement**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Creates Data Diversity\n",
        "\n",
        "Each bootstrap sample contains:\n",
        "\n",
        "* Some original observations repeated\n",
        "* Some observations left out\n",
        "\n",
        "This ensures that every model is trained on slightly different data, creating diversity among models.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Reduces Model Correlation\n",
        "\n",
        "Because each model sees different data, they learn different patterns.\n",
        "This reduces correlation between models, which improves the effectiveness of averaging.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Reduces Variance\n",
        "\n",
        "Since models are trained on varied samples, their errors differ.\n",
        "When predictions are combined (averaging or majority voting), random errors cancel out, reducing variance and overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ Enables Out-of-Bag (OOB) Evaluation\n",
        "\n",
        "The data points not included in a bootstrap sample (called Out-of-Bag samples) can be used to estimate model performance without needing a separate validation set.\n",
        "\n"
      ],
      "metadata": {
        "id": "hcqOqCUXMlSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What are some real-world applications of ensemble techniques?\n",
        "\n",
        "\n",
        "### Real-World Applications of Ensemble Techniques\n",
        "\n",
        "Ensemble techniques (like Bagging, Boosting, and Random Forest) are widely used because they improve accuracy and reduce overfitting. Some important real-world applications are:\n",
        "\n",
        "---\n",
        "\n",
        "### 1Ô∏è‚É£ Finance ‚Äì Fraud Detection\n",
        "\n",
        "Banks use ensemble models to detect fraudulent transactions.\n",
        "By combining multiple models, they can better identify unusual patterns and reduce false positives.\n",
        "\n",
        "---\n",
        "\n",
        "### 2Ô∏è‚É£ Healthcare ‚Äì Disease Prediction\n",
        "\n",
        "Ensemble methods help in diagnosing diseases like cancer or heart disease by analyzing medical data, improving prediction reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ E-Commerce ‚Äì Recommendation Systems\n",
        "\n",
        "Companies like Amazon use ensemble techniques to recommend products based on user behavior and purchase history.\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ Search Engines & Ranking\n",
        "\n",
        "Search engines such as Google use ensemble-based algorithms to improve search result ranking and relevance.\n",
        "\n",
        "---\n",
        "\n",
        "### 5Ô∏è‚É£ Credit Scoring\n",
        "\n",
        "Financial institutions use ensemble models to assess credit risk and decide whether to approve loans.\n",
        "\n",
        "---\n",
        "\n",
        "### 6Ô∏è‚É£ Image & Speech Recognition\n",
        "\n",
        "Ensemble models improve accuracy in applications like facial recognition and voice assistants (e.g., Siri).\n",
        "\n",
        "---\n",
        "\n",
        "### 7Ô∏è‚É£ Stock Market Prediction\n",
        "\n",
        "Boosting and other ensemble techniques are used to analyze historical stock data and predict price movements.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S-GfcN-BM2yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What is the difference between Bagging and Boosting?\n",
        "###  Difference Between Bagging and Boosting\n",
        "\n",
        "Bagging and Boosting are both ensemble learning techniques, but they work in different ways.\n",
        "\n",
        "| Basis                  | Bagging                                                        | Boosting                                                               |\n",
        "| ---------------------- | -------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
        "| **Full Form**          | Bootstrap Aggregating                                          | ‚Äî                                                                      |\n",
        "| **Training Method**    | Models are trained **independently**                           | Models are trained **sequentially**                                    |\n",
        "| **Data Sampling**      | Uses **bootstrap sampling** (random sampling with replacement) | Uses full dataset but gives **higher weight to misclassified samples** |\n",
        "| **Goal**               | Reduces **variance**                                           | Reduces **bias and variance**                                          |\n",
        "| **Overfitting**        | Less prone to overfitting                                      | Can overfit if not tuned properly                                      |\n",
        "| **Model Combination**  | Averaging (regression) / Majority voting (classification)      | Weighted sum of models                                                 |\n",
        "| **Example Algorithms** | Random Forest                                                  | AdaBoost, Gradient Boosting                                            |\n"
      ],
      "metadata": {
        "id": "LkbRSvuyNRF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?\n"
      ],
      "metadata": {
        "id": "aRA4XI4rNjE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create base model (Decision Tree)\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Create Bagging Classifier\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=dt,\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LTQKpk4Ns7Q",
        "outputId": "e2b92e7c-94b4-48b3-f3e9-0b6455adea05"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "kLEnvE6VN0gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create base model (Decision Tree Regressor)\n",
        "dt = DecisionTreeRegressor()\n",
        "\n",
        "# Create Bagging Regressor\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=dt,\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud2DNIjnN4zI",
        "outputId": "32cc8029-56dd-436d-bf57-a26f078b1717"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 2987.0073593984966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "xfADbsOAOE4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBCVCeB-OPal",
        "outputId": "74b6c63f-d071-40c4-ebdd-d8f899da0902"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Feature  Importance\n",
            "7       mean concave points    0.141934\n",
            "27     worst concave points    0.127136\n",
            "23               worst area    0.118217\n",
            "6            mean concavity    0.080557\n",
            "20             worst radius    0.077975\n",
            "22          worst perimeter    0.074292\n",
            "2            mean perimeter    0.060092\n",
            "3                 mean area    0.053810\n",
            "26          worst concavity    0.041080\n",
            "0               mean radius    0.032312\n",
            "13               area error    0.029538\n",
            "21            worst texture    0.018786\n",
            "25        worst compactness    0.017539\n",
            "10             radius error    0.016435\n",
            "28           worst symmetry    0.012929\n",
            "12          perimeter error    0.011770\n",
            "24         worst smoothness    0.011769\n",
            "1              mean texture    0.011064\n",
            "5          mean compactness    0.009216\n",
            "19  fractal dimension error    0.007135\n",
            "29  worst fractal dimension    0.006924\n",
            "4           mean smoothness    0.006223\n",
            "14         smoothness error    0.005881\n",
            "16          concavity error    0.005816\n",
            "15        compactness error    0.004596\n",
            "18           symmetry error    0.004001\n",
            "17     concave points error    0.003382\n",
            "8             mean symmetry    0.003278\n",
            "11            texture error    0.003172\n",
            "9    mean fractal dimension    0.003140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n"
      ],
      "metadata": {
        "id": "bsM_-iMaOSRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree MSE:\", dt_mse)\n",
        "print(\"Random Forest MSE:\", rf_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp7ALlEfOc0G",
        "outputId": "655f4df3-482a-48d1-86f7-0e9e48b3823c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 5697.789473684211\n",
            "Random Forest MSE: 2859.641982706767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n"
      ],
      "metadata": {
        "id": "psEnQI9kOjxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create Random Forest with OOB enabled\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,      # Enable OOB scoring\n",
        "    bootstrap=True,      # Must be True for OOB\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Print OOB Score\n",
        "print(\"Out-of-Bag (OOB) Score:\", rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sx8BxQUOsiq",
        "outputId": "8c89164b-b0a9-466e-cb4e-37a42da8812e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.961335676625659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n"
      ],
      "metadata": {
        "id": "ZYz7HhrBOuX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create base model (SVM)\n",
        "svm = SVC(kernel='rbf', probability=True)\n",
        "\n",
        "# Create Bagging Classifier with SVM\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=svm,\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ513zjQO2Gl",
        "outputId": "84a5b306-e707-4a30-c3e1-866377c03a11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n"
      ],
      "metadata": {
        "id": "3RoPZDBLO40r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Different numbers of trees\n",
        "n_trees = [10, 50, 100, 200]\n",
        "\n",
        "# Train and compare models\n",
        "for n in n_trees:\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Number of Trees: {n}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rpwWPCvPA4N",
        "outputId": "38295478-d9a4-416b-def3-9b4ea3d919b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees: 10, Accuracy: 0.9649\n",
            "Number of Trees: 50, Accuracy: 0.9708\n",
            "Number of Trees: 100, Accuracy: 0.9708\n",
            "Number of Trees: 200, Accuracy: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n"
      ],
      "metadata": {
        "id": "2pfLYzJpPDYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Base model (Logistic Regression)\n",
        "log_reg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# Bagging Classifier with Logistic Regression\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=log_reg,\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = bagging_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute AUC Score\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"AUC Score:\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_32me6bPLih",
        "outputId": "fbff3074-8443-45c1-dae1-ce1f7f236365"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9976484420928865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Train a Random Forest Regressor and analyze feature importance scores.\n"
      ],
      "metadata": {
        "id": "ro9aldOQPSQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for better interpretation\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print feature importance\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwaRJYLXPWO_",
        "outputId": "6d840156-964d-4a2f-fdef-b6085e02c9fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Feature  Importance\n",
            "2     bmi    0.400000\n",
            "8      s5    0.166602\n",
            "3      bp    0.104839\n",
            "9      s6    0.071358\n",
            "6      s3    0.061730\n",
            "0     age    0.058633\n",
            "4      s1    0.049191\n",
            "5      s2    0.047138\n",
            "7      s4    0.029427\n",
            "1     sex    0.011082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n"
      ],
      "metadata": {
        "id": "zqK07HgmPjGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 1. Bagging Classifier\n",
        "# -------------------------\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=dt,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Random Forest Classifier\n",
        "# -------------------------\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY19DDaDPxwh",
        "outputId": "8294b2a8-8ecc-4c95-b7a5-edfae8fd6bc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.9590643274853801\n",
            "Random Forest Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    }
  ]
}